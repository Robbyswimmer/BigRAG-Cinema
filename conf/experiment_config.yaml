# =============================================================================
# BigRAG Cinema -- Experiment Configuration
# =============================================================================
# Central place to define every tuneable knob for benchmark runs.
# Loaded by scripts/run_benchmarks.py (and any notebook that imports the
# experiment harness).
# =============================================================================

# ---- Dataset -----------------------------------------------------------------
# Fractions of the full dataset to benchmark against.  Each fraction produces
# its own set of timing / quality measurements so we can plot scale curves.
dataset:
  source_path: "data/movies_with_embeddings.parquet"
  fractions: [0.01, 0.05, 0.1, 0.25, 0.5, 1.0]

# ---- Embedding model ---------------------------------------------------------
embedding:
  model_name: "all-MiniLM-L6-v2"        # HuggingFace sentence-transformers ID
  dimension: 384                         # output vector length
  batch_size: 256                        # encode batch size on driver
  normalize: true                        # L2-normalize vectors before storage

# ---- Query workload ----------------------------------------------------------
queries:
  num_queries: 50                        # how many random queries per run
  seed: 42                               # reproducibility
  query_source: "data/sample_queries.txt"  # one natural-language query per line

# ---- Retrieval strategies to compare ----------------------------------------
# Each strategy is executed for every (fraction, num_queries) combination.
strategies:
  - name: "brute_force_cosine"
    description: "Exact cosine similarity via Spark UDF"
    top_k: 10

  - name: "broadcast_join"
    description: "Broadcast the query vector, join on cosine similarity"
    top_k: 10

  - name: "partitioned_search"
    description: "Hash-partition embeddings, local top-k per partition, global merge"
    top_k: 10
    num_partitions: 8

  - name: "lsh_approx"
    description: "Spark ML BucketedRandomProjectionLSH approximate NN"
    top_k: 10
    num_hash_tables: 5
    bucket_length: 2.0

# ---- Evaluation metrics ------------------------------------------------------
metrics:
  - "latency_ms"          # wall-clock time for the retrieval call
  - "precision_at_k"      # overlap with brute-force ground truth
  - "recall_at_k"
  - "throughput_qps"       # queries per second

# ---- Output ------------------------------------------------------------------
output:
  results_dir: "results/"
  plots_dir: "results/plots/"
  save_raw_results: true
  file_format: "parquet"    # parquet | csv

# ---- Misc --------------------------------------------------------------------
misc:
  num_repetitions: 3        # repeat each (strategy, fraction) combo for stability
  warm_up_runs: 1           # discarded runs before timing starts
  log_level: "INFO"
